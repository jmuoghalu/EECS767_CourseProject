robot exclus standard wikipedia robot exclus standard wikipedia free encyclopedia jump navig search robot exclus standard also known robot exclus protocol simpli robotstxt standard use websit commun web crawler web robot standard specifi inform web robot area websit process scan robot often use search engin categor websit robot cooper standard email harvest spambot malwar robot scan secur vulner may even start portion websit told stay standard differ use conjunct sitemap robot inclus standard websit content edit standard propos martijn koster 1 2 work nexor 3 februari 1994 4 wwwtalk mail list main commun channel wwwrelat activ time charl stross claim provok koster suggest robotstxt wrote badlybehav web crawler inadvert caus denial servic attack koster server 5 quickli becam de facto standard present futur web crawler expect follow compli includ oper search engin webcrawl lyco altavista 6 edit site owner wish give instruct web robot place text file call root web site hierarchi eg text file contain instruct specif format see exampl robot choos follow instruct tri fetch file read instruct fetch file websit file doesnt exist web robot assum web owner wish provid specif instruct crawl entir site robotstxt file websit function request specifi robot ignor specifi file directori crawl site might exampl prefer privaci search engin result belief content select directori might mislead irrelev categor site whole desir applic oper certain data link page list robotstxt still appear search result link page crawl 7 robotstxt file cover one origin websit multipl subdomain subdomain must robotstxt file robotstxt file rule would appli would appli addit protocol port need robotstxt file appli page major search engin follow standard includ ask 8 aol 9 baidu 10 bing 11 duckduckgo 12 googl 13 yahoo 14 yandex 15 volunt group archiv team explicitli ignor robotstxt part view obsolet standard hinder web archiv effort accord project leader jason scott uncheck left alon robotstxt file ensur mirror refer item may gener use mean beyond websit context 16 year internet archiv crawl site robotstxt april 2017 announc would longer honour direct robotstxt file time observ robotstxt file gear toward search engin crawler necessarili serv archiv purpos 17 respons entir domain tag robotstxt content becam obsolet 17 edit despit use term allow disallow protocol pure advisori 18 reli complianc web robot malici web robot unlik honor robotstxt may even use robotstxt guid find disallow link go straight sometim claim secur risk 19 sort secur obscur discourag standard bodi nation institut standard technolog nist unit state specif recommend practic system secur depend secreci implement compon 20 context robotstxt file secur obscur recommend secur techniqu 21 edit mani robot also pass special userag web server fetch content 22 web administr could also configur server automat return failur pass altern content detect connect use one robot 23 24 site notabl googl host humanstxt file display site contributor inform 25 site github redirect page 26 googl also joke file host killerrobotstxt 27 edit exampl tell robot visit file wildcard stand robot disallow direct valu mean page disallow userag disallow result accomplish empti miss robotstxt file exampl tell robot stay websit userag disallow exampl tell robot enter three directori userag disallow cgibin disallow tmp disallow junk exampl tell robot stay away one specif file userag disallow directoryfilehtml note file specifi directori process exampl tell specif robot stay websit userag badbot replac badbot actual userag bot disallow exampl tell two specif robot enter one specif directori userag badbot replac badbot actual userag bot userag googlebot disallow privat exampl demonstr comment use comment appear symbol start line direct userag match bot disallow keep also possibl list multipl robot rule actual robot string defin crawler robot oper googl support sever userag string allow oper deni access subset servic use specif userag string 13 exampl demonstr multipl userag userag googlebot googl servic disallow privat disallow directori userag googlebotnew news servic disallow disallow everyth userag robot disallow someth disallow directori httpsenwikipediaorgrobotstxt edit edit crawldelay valu support crawler throttl visit host sinc valu part standard interpret depend crawler read yandex interpret valu number second wait subsequ visit 15 bing defin crawldelay size time window 1 30 second bingbot access web site 28 userag crawldelay 10 edit major crawler support allow direct counteract follow disallow direct 29 30 use one tell robot avoid entir directori still want html document directori crawl index standard implement first match robotstxt pattern alway win googl implement differ allow pattern equal charact direct path win match disallow pattern 31 bing use either allow disallow direct whichev specif base length like googl 11 order compat robot one want allow singl file insid otherwis disallow directori necessari place allow direct first follow disallow exampl allow directory1myfilehtml disallow directory1 exampl disallow anyth directory1 except directory1myfilehtml sinc latter match first order import robot follow standard case googl bing bot order import edit crawler support sitemap direct allow multipl sitemap robotstxt form 32 sitemap httpwwwgstaticcoms2sitemapsprofilessitemapxml sitemap httpwwwgooglecomhostednewssitemap_indexxml edit crawler yandex support host direct allow websit multipl mirror specifi prefer domain 33 host freehostingcom altern host 1989shackcom note support crawler use insert bottom file crawldelay direct edit robot exclus standard mention anyth charact disallow statement crawler like googlebot recogn string contain msnbot teoma interpret differ way edit addit rootlevel robotstxt file robot exclus direct appli granular level use robot meta tag xrobotstag http header robot meta tag cannot use nonhtml file imag text file pdf document hand xrobotstag ad nonhtml file use htaccess httpdconf file 34 noindex meta tag noindex http respons header xrobotstag noindex xrobotstag effect page request server respond robot meta tag effect page load wherea robotstxt effect page request thu page exclud robotstxt file robot meta tag xrobotstag header effect ignor robot see first place 34 edit autom content access protocol fail propos extend robotstxt botseer inact search engin robotstxt file distribut web crawl focus crawler internet archiv nation digit librari program ndlp nation digit inform infrastructur preserv program ndiipp nofollow permacc meta element search engin sitemap spider trap web archiv web crawler edit edit robot databas list bot name internet portal retriev httpsenwikipediaorgwindexphptitlerobots_exclusion_standardoldid839506688 categori world wide web hidden categori articl unsourc statement articl unsourc statement novemb 2016 offici websit differ wikidata wikipedia navig menu person tool log talk contribut creat account log namespac view search navig main page content featur content current event random articl donat wikipedia wikipedia store interact help wikipedia commun portal recent chang contact page tool link relat chang upload file special page perman link page inform wikidata item cite page printexport creat book download pdf printabl version languag boarisch catal etina dansk deutsch espaol franai bahasa indonesia italiano nederland polski portugu simpl english srpski suomi svenska trke page last edit 3 may 2018 2158 text avail creativ common attributionsharealik licens addit term may appli use site agre term use privaci polici wikipedia regist trademark wikimedia foundat inc nonprofit organ privaci polici wikipedia disclaim contact wikipedia develop cooki statement mobil view 